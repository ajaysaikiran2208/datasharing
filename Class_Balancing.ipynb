{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class Balancing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoblM4V5BdDbtmo40MFxU5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysaikiran2208/datasharing/blob/master/Class_Balancing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c9VyfAawi4v"
      },
      "source": [
        "**Class** **Balancing in Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iq967SbwpfI"
      },
      "source": [
        "If you have spent some time in machine learning and data science, you would have definitely come across imbalanced class distribution. This is a scenario where the number of observations belonging to one class is significantly lower than those belonging to the other classes.\n",
        "\n",
        "This problem is predominant in scenarios where anomaly detection is crucial like electricity pilferage, fraudulent transactions in banks, identification of rare diseases, etc. In this situation, the predictive model developed using conventional machine learning algorithms could be biased and inaccurate.\n",
        "\n",
        "This happens because Machine Learning Algorithms are usually designed to improve accuracy by reducing the error. Thus, they do not take into account the class distribution / proportion or balance of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDt5euR3whin"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "nb_samples = 1000\n",
        "weights = (0.95, 0.05)\n",
        "x, y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, weights=weights, random_state=1000)\n",
        "\n",
        "print(x[y==0].shape)\n",
        "print(x[y==1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94JL8UY0u9d4"
      },
      "source": [
        "So as expected, the first class is dominant. To balance the classes of this kind of dataset we have two techniques for avoiding class imbalance in machine learning:\n",
        "\n",
        "\n",
        "\n",
        "*Data Level approach: Resampling Techniques*\n",
        "\n",
        "1.Resampling with replacement\n",
        "\n",
        "2.SMOTE Resampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgVXsXVwzWtP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0cldoqCvHr4"
      },
      "source": [
        "Now let’s go through both these class balancing techniques to see how we can balance the classes before using any machine learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCyZ7SSuvV13"
      },
      "source": [
        "## Resampling with Replacement:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDWrYELFvWPP"
      },
      "source": [
        "\n",
        "In the resampling with replacement method, we resample from the dataset limited to the minor class until we reach the desired number of samples in both classes. As we operate with replacing, it can be iterated by the n number of times. But the resulting dataset will contain data points sampled from 54 possible values (according to our example). Here is how we can use the resampling with replacement technique using Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMkYfzJKu2n9",
        "outputId": "61312dc9-6fb0-4a42-e5cf-aad5be4d9324"
      },
      "source": [
        "# Resampling with Replacement\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "x_resampled = resample(x[y==1], n_samples=x[y==0].shape[0], random_state=1000)\n",
        "\n",
        "x_ = np.concatenate((x[y==0], x_resampled))\n",
        "y_ = np.concatenate((y[y==0], np.ones(shape=(x[y==0].shape[0],), dtype=np.int32)))\n",
        "\n",
        "print(x_[y_==0].shape)\n",
        "print(x_[y_==1].shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(946, 2)\n",
            "(946, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdYsG460vtTJ"
      },
      "source": [
        "## SMOTE Resampling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIO7Ks6v8Wf"
      },
      "source": [
        "\n",
        "SMOTE resampling is one of the most robust approaches for avoiding class imbalance. It stands for Synthetic Minority Over-sampling Technique. This technique was designed to generate new samples consistent with the minor classes. To implement the SMOTE resampling technique for class balancing, we can use the imbalanced-learn library which has many algorithms for this kind of problem. Here’s how to implement SMOTE resampling for class balancing using Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNVUn4B1wBwn",
        "outputId": "50466be3-afdc-42f3-f874-1adda4c3d518"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=1000)\n",
        "x_, y_ = smote.fit_sample(x, y)\n",
        "\n",
        "print(x_[y_==0].shape)\n",
        "print(x_[y_==1].shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(946, 2)\n",
            "(946, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kCSNwrOwOg4"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97DaRuViwLzf"
      },
      "source": [
        "\n",
        "Both the Resampling with replacement and SMOTE resampling are very useful techniques for avoiding Class imbalance in machine learning. Resampling with replacement method is used to increase the number of samples but the resulting distribution will be the same as the values are taken from the existing set. Whereas, SMOTE resampling generates the same number of samples by considering the neighbours. "
      ]
    }
  ]
}